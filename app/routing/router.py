"""
Core fallback router implementation.
Handles multi-level routing with API key, provider, and logical model fallbacks.
Enhanced with RouteExecutor integration and streaming support.
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator, Callable, Dict, List, Literal, Optional

import aiohttp

from app.core.api_key_manager import (
    KeyCycleTracker,
    get_available_keys,
)
from app.core.provider_config import get_provider_wire_protocol, get_provider_config
from app.routing.config_loader import config_loader
from app.routing.executor import RouteExecutionError, RouteExecutor, get_executor
from app.routing.models import (
    Attempt,
    ResolvedRoute,
    RoutingError,
    RouteConfig,
    ModelRoutingConfig,
)

logger = logging.getLogger("fallback_router")

# Verbose error logging (shows full error body)
VERBOSE_HTTP_ERRORS = os.getenv("VERBOSE_HTTP_ERRORS", "false").lower() == "true"


def _fix_missing_tool_responses(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Scan messages for tool_calls without matching tool responses.
    Inject empty tool response messages for each missing tool_call_id.

    This fixes the 422 error: "An assistant message with 'tool_calls' must be
    followed by tool messages responding to each 'tool_call_id'."

    Args:
        request_data: The original request data with messages

    Returns:
        Modified request data with missing tool responses injected
    """
    messages = request_data.get("messages", [])
    if not messages:
        return request_data

    fixed_messages = []
    pending_tool_call_ids = []

    for msg in messages:
        # If assistant message has tool_calls, track the IDs
        if msg.get("role") == "assistant" and msg.get("tool_calls"):
            # First, check if there are still pending tool calls from before
            # that need responses injected
            if pending_tool_call_ids:
                for tc_id in pending_tool_call_ids:
                    fixed_messages.append(
                        {"role": "tool", "tool_call_id": tc_id, "content": ""}
                    )
                pending_tool_call_ids.clear()

            # Now add the current assistant message
            fixed_messages.append(msg)

            # Track new pending tool_call_ids
            for tc in msg["tool_calls"]:
                tc_id = tc.get("id")
                if tc_id:
                    pending_tool_call_ids.append(tc_id)
        elif msg.get("role") == "tool":
            # Tool response - remove from pending if it matches
            tool_call_id = msg.get("tool_call_id")
            if tool_call_id in pending_tool_call_ids:
                pending_tool_call_ids.remove(tool_call_id)
            fixed_messages.append(msg)
        else:
            # For other messages (user, system), inject any pending tool responses first
            if pending_tool_call_ids:
                for tc_id in pending_tool_call_ids:
                    fixed_messages.append(
                        {"role": "tool", "tool_call_id": tc_id, "content": ""}
                    )
                pending_tool_call_ids.clear()
            fixed_messages.append(msg)

    # Handle any remaining pending tool calls at the end
    if pending_tool_call_ids:
        for tc_id in pending_tool_call_ids:
            fixed_messages.append(
                {"role": "tool", "tool_call_id": tc_id, "content": ""}
            )

    return {**request_data, "messages": fixed_messages}


def _fix_missing_tool_results_anthropic(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Anthropic-format variant of the tool-response auto-fix.

    If an assistant message contains tool_use blocks, ensure that there is a
    subsequent user message containing tool_result blocks for each tool_use id.
    Missing tool_result blocks are fabricated with empty content.

    This is used when the client is using the Anthropic protocol but the upstream
    provider is OpenAI-compatible (e.g. Cerebras) and rejects missing tool results.
    """
    messages = request_data.get("messages", [])
    if not isinstance(messages, list) or not messages:
        return request_data

    def _extract_tool_use_ids(content: Any) -> List[str]:
        ids: List[str] = []
        if isinstance(content, list):
            for block in content:
                if isinstance(block, dict) and block.get("type") == "tool_use":
                    tid = block.get("id")
                    if tid:
                        ids.append(str(tid))
        return ids

    def _extract_tool_result_ids(content: Any) -> List[str]:
        ids: List[str] = []
        if isinstance(content, list):
            for block in content:
                if isinstance(block, dict) and block.get("type") == "tool_result":
                    tid = block.get("tool_use_id")
                    if tid:
                        ids.append(str(tid))
        return ids

    def _make_tool_result_message(tool_ids: List[str]) -> Dict[str, Any]:
        return {
            "role": "user",
            "content": [
                {
                    "type": "tool_result",
                    "tool_use_id": tid,
                    "content": "",
                    "is_error": False,
                }
                for tid in tool_ids
            ],
        }

    fixed_messages: List[Dict[str, Any]] = []
    pending: List[str] = []

    for msg in messages:
        if not isinstance(msg, dict):
            continue

        role = msg.get("role")
        content = msg.get("content")

        if role == "assistant":
            # If we have unresolved tool_use ids from a previous assistant message,
            # inject fabricated tool_result blocks before the next assistant turn.
            if pending:
                fixed_messages.append(_make_tool_result_message(pending))
                pending = []

            fixed_messages.append(msg)
            pending = _extract_tool_use_ids(content)
            continue

        if role == "user":
            # If the previous assistant had tool_use blocks, ensure their results exist
            if pending:
                resolved = set(_extract_tool_result_ids(content))
                missing = [tid for tid in pending if tid not in resolved]

                # Inject missing tool_result blocks BEFORE the user's message so that
                # after Anthropic->OpenAI conversion, tool messages come immediately
                # after the assistant tool_calls.
                if missing:
                    fixed_messages.append(_make_tool_result_message(missing))
                pending = []

            fixed_messages.append(msg)
            continue

        # Unknown role: flush pending to keep sequence valid
        if pending:
            fixed_messages.append(_make_tool_result_message(pending))
            pending = []
        fixed_messages.append(msg)

    # If request ends with assistant tool_use blocks and no user tool_result, append empty ones.
    if pending:
        fixed_messages.append(_make_tool_result_message(pending))

    return {**request_data, "messages": fixed_messages}


def _format_error_for_log(
    error: Exception,
    provider: str,
    model: str,
    api_key: Optional[str] = None,
) -> str:
    """
    Format an error for logging, with concise or verbose output.

    For 401 errors, includes last 4 chars of API key for debugging.
    Verbose mode (VERBOSE_HTTP_ERRORS=true) shows full error body.

    Args:
        error: The exception that occurred
        provider: Provider name
        model: Model name
        api_key: Optional API key (last 4 chars shown for 401 errors)

    Returns:
        Formatted error string for logging
    """
    status_code = None
    error_code = None

    # Extract status code from various error types
    if hasattr(error, "status"):
        status_code = error.status
    elif hasattr(error, "status_code"):
        status_code = error.status_code

    # Try to extract error code from body for concise display
    if hasattr(error, "body") and error.body:
        try:
            body_json = json.loads(error.body)
            # Handle list responses (e.g., from Gemini: [{"error": {...}}])
            if isinstance(body_json, list) and body_json:
                body_json = body_json[0]
            if isinstance(body_json, dict):
                error_code = body_json.get("code") or body_json.get("type")
        except (json.JSONDecodeError, TypeError, AttributeError):
            pass

    # Build concise error message
    parts = [f"provider={provider}", f"model={model}"]

    if status_code:
        parts.append(f"status={status_code}")

    # For 401 errors, show last 4 chars of API key
    if status_code == 401 and api_key:
        key_hint = f"...{api_key[-4:]}" if len(api_key) >= 4 else "***"
        parts.append(f"key={key_hint}")

    if error_code:
        parts.append(f"code={error_code}")

    base_msg = ", ".join(parts)

    # Verbose mode: include full error
    if VERBOSE_HTTP_ERRORS:
        return f"{base_msg}, error={str(error)}"

    return base_msg


# Type alias for wire protocols
WireProtocol = Literal["openai", "anthropic"]


class FallbackRouter:
    """
    Router that implements multi-level fallback logic.

    Supports three levels of fallback:
    1. API key fallback - try different API keys for the same provider/model
    2. Provider fallback - try different providers for the same logical model
    3. Model fallback - try different logical models entirely

    Uses RouteExecutor for actual route execution with protocol conversion.
    """

    def __init__(self, executor: Optional[RouteExecutor] = None):
        """
        Initialize the FallbackRouter.

        Args:
            executor: Optional RouteExecutor instance (uses singleton if not provided)
        """
        self._visited_models: set[str] = set()  # Track visited models to prevent cycles
        self._executor = executor or get_executor()
        self._model_config_cache: Dict[str, ModelRoutingConfig] = {}

    def resolve_error_action(self, provider_name: str, error: Exception) -> dict:
        """
        Resolve the action to take for a given error based on provider config.

        Default behavior (if not configured in provider error_handling):
        - 401, 403: global_key_failure
        - everything else: model_key_failure

        NOTE: Providers can override specific status codes in their JSON config
        (e.g. map 400/500 to fallback_no_cooldown, or 422 to auto_fix_tool_responses).
        """
        status_code = None
        if hasattr(error, "status"):
            status_code = error.status
        elif hasattr(error, "status_code"):
            status_code = error.status_code
        elif isinstance(error, RouteExecutionError) and error.status_code:
            status_code = error.status_code

        if status_code is None:
            return {"action": "model_key_failure"}

        config = get_provider_config(provider_name)
        error_handling = (config or {}).get("error_handling", {})

        # Look up specific status code action from provider config
        action_info = error_handling.get(str(status_code))
        if action_info:
            return action_info

        # Default behavior based on status code
        if status_code in (401, 403):
            return {"action": "global_key_failure"}

        return {"action": "model_key_failure"}

    def _create_tracker_for_route(
        self,
        route_config: RouteConfig,
        model_config: ModelRoutingConfig,
        max_key_cycles: Optional[int] = None,
    ) -> KeyCycleTracker:
        """
        Create a KeyCycleTracker with granular cooldown settings.
        """
        provider_config = get_provider_config(route_config.provider) or {}

        # 1. Get provider default cooldown
        provider_cooldown = provider_config.get("rate_limiting", {}).get(
            "cooldown_seconds"
        )

        # 2. Get provider-model specific cooldown
        provider_models = provider_config.get("models", {})
        model_specific = provider_models.get(route_config.model, {})
        model_cooldown = model_specific.get("cooldown_seconds")

        # 3. Get route-level override or model default
        route_cooldown = (
            route_config.cooldown_seconds
            or model_cooldown
            or model_config.default_cooldown_seconds
        )

        return KeyCycleTracker(
            provider=route_config.provider,
            model=route_config.model,
            max_cycles=max_key_cycles,
            provider_cooldown=provider_cooldown,
            route_cooldown=route_cooldown,
        )

    async def call_with_fallback(
        self,
        logical_model: str,
        request_data: Dict[str, Any],
        target_protocol: WireProtocol,
        stream: bool = False,
        max_key_cycles: Optional[int] = None,
    ) -> Any:
        """
        Execute a request with full fallback routing.

        This method uses KeyCycleTracker for each provider to implement
        round-robin key selection with configurable cycle limits.

        Args:
            logical_model: The logical model name (e.g., 'glm-4.6')
            request_data: Request payload in target_protocol format
            target_protocol: The protocol the client expects ("openai" or "anthropic")
            stream: Whether to return a streaming response
            max_key_cycles: Maximum cycles through all keys per provider before
                           falling back to next provider. Defaults to MAX_KEY_RETRY_CYCLES.

        Returns:
            For non-streaming: Response dictionary in target_protocol format
            For streaming: AsyncGenerator yielding SSE chunks

        Raises:
            RoutingError: If all routes fail
        """
        # Collect route configs from model and its fallbacks
        route_configs = self._collect_route_configs(logical_model)

        if not route_configs:
            raise RoutingError(
                logical_model=logical_model,
                attempted_routes=[],
                errors=[
                    {"error": "No routes available", "error_type": "NoRoutesError"}
                ],
                message=f"No routes available for logical model '{logical_model}'",
            )

        errors = []
        all_attempts = []  # Track all attempts for error reporting
        attempt_number = 1

        if stream:
            return self._stream_with_fallback_dynamic(
                route_configs=route_configs,
                request_data=request_data,
                target_protocol=target_protocol,
                logical_model=logical_model,
                max_key_cycles=max_key_cycles,
            )

        # Non-streaming: iterate through route configs with KeyCycleTracker
        for route_config, is_fallback, source_model in route_configs:
            if source_model not in self._model_config_cache:
                self._model_config_cache[source_model] = config_loader.load_config(
                    source_model
                )
            model_config = self._model_config_cache[source_model]

            tracker = self._create_tracker_for_route(
                route_config=route_config,
                model_config=model_config,
                max_key_cycles=max_key_cycles,
            )

            # Skip provider if all keys are in cooldown from previous failures
            if tracker.all_keys_in_cooldown():
                logger.info(
                    f"Skipping provider {route_config.provider}: "
                    f"all {tracker.total_keys} keys are in cooldown"
                )
                continue

            while not tracker.exhausted():
                api_key = tracker.get_next_key()
                if api_key is None:
                    break  # No more keys available for this provider

                resolved_route = self._build_resolved_route(
                    route_config=route_config,
                    source_logical_model=source_model,
                    api_key=api_key,
                )

                attempt = Attempt(
                    route=resolved_route,
                    attempt_number=attempt_number,
                    is_fallback_route=is_fallback,
                )
                all_attempts.append(attempt)

                logger.info(
                    f"Attempting route {attempt_number}: "
                    f"provider={resolved_route.provider}, model={resolved_route.model}, "
                    f"is_fallback={is_fallback}, cycle={tracker.current_cycle}"
                )

                try:
                    result = await self._executor.execute(
                        route=resolved_route,
                        request_data=request_data,
                        target_protocol=target_protocol,
                    )
                    logger.info(
                        f"Route succeeded: provider={resolved_route.provider}, "
                        f"model={resolved_route.model}"
                    )
                    # Print success message to console for visibility
                    print(
                        f"[OK] Request succeeded: {logical_model} -> "
                        f"{resolved_route.provider}/{resolved_route.model} "
                        f"(attempt {attempt_number})"
                    )
                    return result

                except Exception as e:
                    action_info = self.resolve_error_action(resolved_route.provider, e)
                    action = action_info.get("action", "model_key_failure")

                    error_info = {
                        "attempt": attempt_number,
                        "provider": resolved_route.provider,
                        "model": resolved_route.model,
                        "error": str(e),
                        "error_type": type(e).__name__,
                    }

                    # Handle fallback_no_cooldown: just fallback without marking key failed
                    if action == "fallback_no_cooldown":
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=resolved_route.api_key,
                        )
                        logger.info(
                            f"Route failed, falling back (no cooldown): {err_msg}"
                        )
                        errors.append(error_info)
                        attempt_number += 1
                        break

                    # Handle auto_fix_tool_responses: fix request and retry same route
                    if action == "auto_fix_tool_responses":
                        logger.info(
                            f"Attempting auto-fix for missing tool responses on "
                            f"{resolved_route.provider}/{resolved_route.model}"
                        )
                        fixed_request = (
                            _fix_missing_tool_results_anthropic(request_data)
                            if target_protocol == "anthropic"
                            else _fix_missing_tool_responses(request_data)
                        )
                        try:
                            result = await self._executor.execute(
                                route=resolved_route,
                                request_data=fixed_request,
                                target_protocol=target_protocol,
                            )
                            logger.info(
                                f"Route succeeded after auto-fix: "
                                f"provider={resolved_route.provider}, "
                                f"model={resolved_route.model}"
                            )
                            print(
                                f"[OK] Request succeeded (auto-fixed): {logical_model} -> "
                                f"{resolved_route.provider}/{resolved_route.model} "
                                f"(attempt {attempt_number})"
                            )
                            return result
                        except Exception as retry_error:
                            # Auto-fix retry failed, fallback without cooldown
                            err_msg = _format_error_for_log(
                                retry_error,
                                resolved_route.provider,
                                resolved_route.model,
                                api_key=resolved_route.api_key,
                            )
                            logger.warning(
                                f"Auto-fix retry failed, falling back: {err_msg}"
                            )
                            error_info["error"] = str(retry_error)
                            errors.append(error_info)
                            attempt_number += 1
                            break

                    # For other actions, mark key/provider failed
                    tracker.mark_failed(
                        api_key,
                        action=action,
                        cooldown_duration=action_info.get("cooldown_seconds"),
                    )

                    if self._is_fallback_worthy_error(e):
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=resolved_route.api_key,
                        )
                        logger.warning(f"Route failed (fallback): {err_msg}")
                        errors.append(error_info)
                        attempt_number += 1
                        continue
                    else:
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=resolved_route.api_key,
                        )
                        logger.error(f"Route failed (non-recoverable): {err_msg}")
                        raise

                attempt_number += 1

        # All providers exhausted
        error_message = self._format_routing_error_message(
            logical_model, all_attempts, errors
        )
        logger.error(
            f"All routes failed for '{logical_model}': {len(errors)} attempts failed"
        )

        raise RoutingError(
            logical_model=logical_model,
            attempted_routes=all_attempts,
            errors=errors,
            message=error_message,
        )

    def _collect_route_configs(self, logical_model: str) -> List[tuple]:
        """
        Collect all route configs from a model and its fallbacks.

        Returns:
            List of tuples: (RouteConfig, is_fallback: bool, source_model: str)
        """
        self._visited_models.clear()
        return self._collect_route_configs_recursive(logical_model, is_fallback=False)

    def _collect_route_configs_recursive(
        self,
        logical_model: str,
        is_fallback: bool = False,
    ) -> List[tuple]:
        """
        Recursively collect route configs, including fallback models.
        """
        if logical_model in self._visited_models:
            logger.debug(f"Skipping already-visited model: {logical_model}")
            return []

        self._visited_models.add(logical_model)

        try:
            config = config_loader.load_config(logical_model)
        except Exception as e:
            logger.warning(f"Failed to load config for '{logical_model}': {e}")
            self._visited_models.discard(logical_model)
            return []

        routes = []

        # Add routes from this model
        for route_config in config.model_routings:
            routes.append((route_config, is_fallback, logical_model))

        # Recursively add routes from fallback models
        for fallback_model in config.fallback_model_routings:
            fallback_routes = self._collect_route_configs_recursive(
                fallback_model, is_fallback=True
            )
            routes.extend(fallback_routes)

        self._visited_models.discard(logical_model)
        return routes

    def _build_resolved_route(
        self,
        route_config,
        source_logical_model: str,
        api_key: str,
    ) -> ResolvedRoute:
        """
        Build a ResolvedRoute from a route config and API key.
        """
        wire_protocol = route_config.wire_protocol or get_provider_wire_protocol(
            route_config.provider
        )
        return ResolvedRoute(
            source_logical_model=source_logical_model,
            wire_protocol=wire_protocol,
            provider=route_config.provider,
            model=route_config.model,
            base_url=route_config.base_url,
            api_key=api_key,
            timeout_seconds=route_config.timeout_seconds or 60,
        )

    # Legacy method - keep for backward compatibility with resolve_attempts
    async def _call_with_fallback_legacy(
        self,
        logical_model: str,
        request_data: Dict[str, Any],
        target_protocol: WireProtocol,
        stream: bool = False,
    ) -> Any:
        """Legacy implementation using pre-computed attempts."""
        attempts = self.resolve_attempts(logical_model)

        if not attempts:
            raise RoutingError(
                logical_model=logical_model,
                attempted_routes=[],
                errors=[
                    {"error": "No routes available", "error_type": "NoRoutesError"}
                ],
                message=f"No routes available for logical model '{logical_model}'",
            )

        errors = []

        if stream:
            # For streaming, return a generator that handles fallback internally
            return self._stream_with_fallback(
                attempts=attempts,
                request_data=request_data,
                target_protocol=target_protocol,
                logical_model=logical_model,
            )

        # Non-streaming request - try each attempt
        for attempt in attempts:
            logger.info(
                f"Attempting route {attempt.attempt_number}: "
                f"provider={attempt.route.provider}, model={attempt.route.model}, "
                f"is_fallback={attempt.is_fallback_route}"
            )

            try:
                result = await self._executor.execute(
                    route=attempt.route,
                    request_data=request_data,
                    target_protocol=target_protocol,
                )
                logger.info(
                    f"Route succeeded: provider={attempt.route.provider}, "
                    f"model={attempt.route.model}"
                )
                # Print success message to console for visibility
                print(
                    f"[OK] Request succeeded: {logical_model} -> "
                    f"{attempt.route.provider}/{attempt.route.model} "
                    f"(attempt {attempt.attempt_number})"
                )
                return result

            except Exception as e:
                error_info = {
                    "attempt": attempt.attempt_number,
                    "provider": attempt.route.provider,
                    "model": attempt.route.model,
                    "error": str(e),
                    "error_type": type(e).__name__,
                }

                # Check if this is a fallback-worthy error
                if self._is_fallback_worthy_error(e):
                    err_msg = _format_error_for_log(
                        e,
                        attempt.route.provider,
                        attempt.route.model,
                        api_key=attempt.route.api_key,
                    )
                    logger.warning(f"Route failed (fallback): {err_msg}")
                    errors.append(error_info)
                    continue
                else:
                    # Non-fallback error, re-raise immediately
                    err_msg = _format_error_for_log(
                        e,
                        attempt.route.provider,
                        attempt.route.model,
                        api_key=attempt.route.api_key,
                    )
                    logger.error(f"Route failed (non-recoverable): {err_msg}")
                    raise

        # All attempts failed
        error_message = self._format_routing_error_message(
            logical_model, attempts, errors
        )
        logger.error(f"All routes failed for '{logical_model}': {len(errors)} attempts")

        raise RoutingError(
            logical_model=logical_model,
            attempted_routes=attempts,
            errors=errors,
            message=error_message,
        )

    async def _stream_with_fallback_dynamic(
        self,
        route_configs: List[tuple],
        request_data: Dict[str, Any],
        target_protocol: WireProtocol,
        logical_model: str,
        max_key_cycles: Optional[int] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Execute streaming request with dynamic key selection and fallback support.

        Uses KeyCycleTracker for each provider to implement round-robin
        key selection with configurable cycle limits.
        """
        errors = []
        all_attempts = []
        attempt_number = 1

        for route_config, is_fallback, source_model in route_configs:
            if source_model not in self._model_config_cache:
                self._model_config_cache[source_model] = config_loader.load_config(
                    source_model
                )
            model_config = self._model_config_cache[source_model]

            tracker = self._create_tracker_for_route(
                route_config=route_config,
                model_config=model_config,
                max_key_cycles=max_key_cycles,
            )

            # Skip provider if all keys are in cooldown from previous failures
            if tracker.all_keys_in_cooldown():
                logger.info(
                    f"Skipping provider {route_config.provider}: "
                    f"all {tracker.total_keys} keys are in cooldown"
                )
                continue

            while not tracker.exhausted():
                api_key = tracker.get_next_key()
                if api_key is None:
                    break  # No more keys available for this provider

                resolved_route = self._build_resolved_route(
                    route_config=route_config,
                    source_logical_model=source_model,
                    api_key=api_key,
                )

                attempt = Attempt(
                    route=resolved_route,
                    attempt_number=attempt_number,
                    is_fallback_route=is_fallback,
                )
                all_attempts.append(attempt)

                logger.info(
                    f"Attempting streaming route {attempt_number}: "
                    f"provider={resolved_route.provider}, model={resolved_route.model}, "
                    f"is_fallback={is_fallback}, cycle={tracker.current_cycle}"
                )

                try:
                    stream_gen = self._executor.execute_stream(
                        route=resolved_route,
                        request_data=request_data,
                        target_protocol=target_protocol,
                    )

                    async for chunk in stream_gen:
                        yield chunk

                    logger.info(
                        f"Streaming route succeeded: provider={resolved_route.provider}, "
                        f"model={resolved_route.model}"
                    )
                    # Print success message to console for visibility
                    print(
                        f"[OK] Stream succeeded: {logical_model} -> "
                        f"{resolved_route.provider}/{resolved_route.model} "
                        f"(attempt {attempt_number})"
                    )
                    return  # Success!

                except Exception as e:
                    action_info = self.resolve_error_action(resolved_route.provider, e)
                    action = action_info.get("action", "model_key_failure")

                    error_info = {
                        "attempt": attempt_number,
                        "provider": resolved_route.provider,
                        "model": resolved_route.model,
                        "error": str(e),
                        "error_type": type(e).__name__,
                    }

                    # Handle fallback_no_cooldown: just fallback without marking key failed
                    if action == "fallback_no_cooldown":
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=api_key,
                        )
                        logger.info(
                            f"Stream failed, falling back (no cooldown): {err_msg}"
                        )
                        errors.append(error_info)
                        attempt_number += 1
                        break

                    # Handle auto_fix_tool_responses: fix request and retry same route
                    if action == "auto_fix_tool_responses":
                        logger.info(
                            f"Attempting auto-fix for missing tool responses on "
                            f"{resolved_route.provider}/{resolved_route.model}"
                        )
                        fixed_request = (
                            _fix_missing_tool_results_anthropic(request_data)
                            if target_protocol == "anthropic"
                            else _fix_missing_tool_responses(request_data)
                        )
                        try:
                            stream_gen = self._executor.execute_stream(
                                route=resolved_route,
                                request_data=fixed_request,
                                target_protocol=target_protocol,
                            )
                            async for chunk in stream_gen:
                                yield chunk
                            logger.info(
                                f"Streaming route succeeded after auto-fix: "
                                f"provider={resolved_route.provider}, "
                                f"model={resolved_route.model}"
                            )
                            print(
                                f"[OK] Stream succeeded (auto-fixed): {logical_model} -> "
                                f"{resolved_route.provider}/{resolved_route.model} "
                                f"(attempt {attempt_number})"
                            )
                            return  # Success!
                        except Exception as retry_error:
                            # Auto-fix retry failed, fallback without cooldown
                            err_msg = _format_error_for_log(
                                retry_error,
                                resolved_route.provider,
                                resolved_route.model,
                                api_key=api_key,
                            )
                            logger.warning(
                                f"Auto-fix retry failed, falling back: {err_msg}"
                            )
                            error_info["error"] = str(retry_error)
                            errors.append(error_info)
                            attempt_number += 1
                            break

                    # For other actions, mark key/provider failed
                    tracker.mark_failed(
                        api_key,
                        action=action,
                        cooldown_duration=action_info.get("cooldown_seconds"),
                    )

                    if self._is_fallback_worthy_error(e):
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=api_key,
                        )
                        logger.warning(f"Stream failed (fallback): {err_msg}")
                        errors.append(error_info)
                        attempt_number += 1
                        continue
                    else:
                        err_msg = _format_error_for_log(
                            e,
                            resolved_route.provider,
                            resolved_route.model,
                            api_key=api_key,
                        )
                        logger.error(f"Stream failed (non-recoverable): {err_msg}")
                        raise

                attempt_number += 1

        # All providers exhausted
        error_message = self._format_routing_error_message(
            logical_model, all_attempts, errors
        )
        logger.error(f"All routes failed for '{logical_model}': {len(errors)} attempts")

        raise RoutingError(
            logical_model=logical_model,
            attempted_routes=all_attempts,
            errors=errors,
            message=error_message,
        )

    async def _stream_with_fallback(
        self,
        attempts: List[Attempt],
        request_data: Dict[str, Any],
        target_protocol: WireProtocol,
        logical_model: str,
    ) -> AsyncGenerator[str, None]:
        """
        Legacy streaming implementation using pre-computed attempts.

        Kept for backward compatibility with resolve_attempts().
        """
        errors = []

        for attempt in attempts:
            logger.info(
                f"Attempting streaming route {attempt.attempt_number}: "
                f"provider={attempt.route.provider}, model={attempt.route.model}, "
                f"is_fallback={attempt.is_fallback_route}"
            )

            try:
                # Get the stream generator
                stream_gen = self._executor.execute_stream(
                    route=attempt.route,
                    request_data=request_data,
                    target_protocol=target_protocol,
                )

                async for chunk in stream_gen:
                    yield chunk

                # Stream completed successfully
                logger.info(
                    f"Streaming route succeeded: provider={attempt.route.provider}, "
                    f"model={attempt.route.model}"
                )
                # Print success message to console for visibility
                print(
                    f"[OK] Stream succeeded: {logical_model} -> "
                    f"{attempt.route.provider}/{attempt.route.model} "
                    f"(attempt {attempt.attempt_number})"
                )
                return  # Exit the generator

            except Exception as e:
                error_info = {
                    "attempt": attempt.attempt_number,
                    "provider": attempt.route.provider,
                    "model": attempt.route.model,
                    "error": str(e),
                    "error_type": type(e).__name__,
                }

                # Check if this is a fallback-worthy error
                if self._is_fallback_worthy_error(e):
                    err_msg = _format_error_for_log(
                        e,
                        attempt.route.provider,
                        attempt.route.model,
                        api_key=attempt.route.api_key,
                    )
                    logger.warning(f"Stream failed (fallback): {err_msg}")
                    errors.append(error_info)
                    continue  # Try next attempt
                else:
                    # Non-fallback error, re-raise
                    err_msg = _format_error_for_log(
                        e,
                        attempt.route.provider,
                        attempt.route.model,
                        api_key=attempt.route.api_key,
                    )
                    logger.error(f"Stream failed (non-recoverable): {err_msg}")
                    raise

        # All attempts failed
        error_message = self._format_routing_error_message(
            logical_model, attempts, errors
        )
        logger.error(f"All routes failed for '{logical_model}': {len(errors)} attempts")

        raise RoutingError(
            logical_model=logical_model,
            attempted_routes=attempts,
            errors=errors,
            message=error_message,
        )

    def resolve_attempts(self, logical_model: str) -> List[Attempt]:
        """
        Resolve all possible attempts for a logical model, including fallbacks.

        This builds an ordered list of all routes to try, including:
        - All API keys for each provider route in the model config
        - Routes from fallback logical models

        Returns:
            Ordered list of Attempt objects to try in sequence
        """
        self._visited_models.clear()
        return self._resolve_attempts_recursive(logical_model, is_fallback=False)

    def _resolve_attempts_recursive(
        self, logical_model: str, is_fallback: bool = False
    ) -> List[Attempt]:
        """
        Recursively resolve attempts, including fallback logical models.

        Args:
            logical_model: Model name to resolve
            is_fallback: Whether this is a fallback model (affects logging/tracking)

        Returns:
            List of Attempt objects
        """
        # Cycle detection
        if logical_model in self._visited_models:
            logger.debug(f"Skipping already-visited model: {logical_model}")
            return []

        self._visited_models.add(logical_model)

        try:
            config = config_loader.load_config(logical_model)
        except Exception as e:
            logger.warning(f"Failed to load config for '{logical_model}': {e}")
            self._visited_models.discard(logical_model)
            return []

        attempts = []
        attempt_number = 1

        # First, add attempts from this model's routes
        for route_config in config.model_routings:
            route_attempts = self._expand_route_to_attempts(
                route_config=route_config,
                source_logical_model=logical_model,
                default_timeout=config.timeout_seconds or 60,
                start_attempt_number=attempt_number,
                is_fallback_route=is_fallback,
            )
            attempts.extend(route_attempts)
            attempt_number += len(route_attempts)

        # Then recursively add attempts from fallback models
        for fallback_model in config.fallback_model_routings:
            fallback_attempts = self._resolve_attempts_recursive(
                fallback_model, is_fallback=True
            )
            # Renumber the fallback attempts
            for attempt in fallback_attempts:
                attempt.attempt_number = attempt_number
                attempt_number += 1
            attempts.extend(fallback_attempts)

        self._visited_models.discard(logical_model)
        return attempts

    def _expand_route_to_attempts(
        self,
        route_config,
        source_logical_model: str,
        default_timeout: int,
        start_attempt_number: int,
        is_fallback_route: bool,
    ) -> List[Attempt]:
        """
        Expand a single route config into multiple attempts (one per API key).

        Each API key in the route config becomes a separate attempt,
        allowing for API key-level fallback.

        Args:
            route_config: The route configuration
            source_logical_model: The logical model this route belongs to
            default_timeout: Default timeout if not specified in route
            start_attempt_number: Starting attempt number for this batch
            is_fallback_route: Whether this is from a fallback model

        Returns:
            List of Attempt objects, one per available API key
        """
        attempts = []
        attempt_offset = 0

        wire_protocol = route_config.wire_protocol or get_provider_wire_protocol(
            route_config.provider
        )

        if route_config.api_key_env:
            api_keys = []
            for api_key_env in route_config.api_key_env:
                api_key = os.getenv(api_key_env)
                if not api_key:
                    logger.debug(f"API key not found in env var: {api_key_env}")
                    continue
                api_keys.append(api_key)
        else:
            api_keys = get_available_keys(route_config.provider)

        for api_key in api_keys:
            resolved_route = ResolvedRoute(
                source_logical_model=source_logical_model,
                wire_protocol=wire_protocol,
                provider=route_config.provider,
                model=route_config.model,
                base_url=route_config.base_url,
                api_key=api_key,
                timeout_seconds=route_config.timeout_seconds or default_timeout,
            )

            attempt = Attempt(
                route=resolved_route,
                attempt_number=start_attempt_number + attempt_offset,
                is_fallback_route=is_fallback_route,
            )

            attempts.append(attempt)
            attempt_offset += 1

        return attempts

    def _is_fallback_worthy_error(self, error: BaseException) -> bool:
        """
        Determine if an error should trigger fallback to the next route.

        Fallback-worthy errors include:
        - HTTP 4xx/5xx status codes (except 401/403 which are permanent auth issues)
        - Network errors (connection, DNS, timeout, etc.)
        - Rate limiting (429)
        - Provider-specific transient errors

        Args:
            error: The exception to evaluate

        Returns:
            True if fallback should be attempted, False if error is permanent
        """
        # Check for HTTP status errors
        if hasattr(error, "status"):
            status = error.status
            if status is not None:
                # 401/403 are auth errors - likely permanent issues with this key
                # Don't fallback to same provider with different key for auth errors
                if status in (401, 403):
                    return True  # Still allow fallback to different provider/key
                # Other 4xx and 5xx errors are fallback-worthy
                if 400 <= status < 600:
                    return True

        # Check for RouteExecutionError
        if isinstance(error, RouteExecutionError):
            if error.status_code:
                return 400 <= error.status_code < 600
            return True  # Default to allowing fallback for execution errors

        # Check for aiohttp client errors (network issues)
        if isinstance(
            error,
            (aiohttp.ClientError, aiohttp.ServerTimeoutError, asyncio.TimeoutError),
        ):
            return True

        # Check for httpx timeout errors
        try:
            import httpx

            if isinstance(error, (httpx.TimeoutException, httpx.ConnectError)):
                return True
        except ImportError:
            pass

        # Check error message for transient indicators
        error_msg = str(error).lower()
        transient_indicators = [
            "timeout",
            "connection",
            "network",
            "unreachable",
            "reset",
            "server error",
            "internal server error",
            "bad gateway",
            "service unavailable",
            "gateway timeout",
            "too many requests",
            "rate limit",
            "temporarily",
            "overloaded",
            "capacity",
        ]

        return any(indicator in error_msg for indicator in transient_indicators)

    def _is_global_error(self, error: Exception) -> bool:
        """
        Determine if an error is global (applies to all models for this key)
        or model-specific.

        Global errors: 401 Unauthorized, 403 Forbidden (key-wide authentication issues).
        Model-specific: 400 Bad Request (could be model params), 404 Not Found,
                        429 Rate Limit, 5xx Server Error.
        """
        status_code = None
        if hasattr(error, "status"):
            status_code = error.status
        elif hasattr(error, "status_code"):
            status_code = error.status_code
        elif isinstance(error, RouteExecutionError) and error.status_code:
            status_code = error.status_code

        # Only 401 and 403 are truly key-wide/account-wide issues.
        # 400 is ambiguous (could be model-specific params) so treat as model-specific.
        return status_code in (401, 403)

    def _format_routing_error_message(
        self,
        logical_model: str,
        attempts: List[Attempt],
        errors: List[Dict[str, Any]],
    ) -> str:
        """
        Format a human-readable error message for routing failures.

        Args:
            logical_model: The logical model that was requested
            attempts: List of all attempts made
            errors: List of error details from failed attempts

        Returns:
            Formatted error message
        """
        parts = [f"All routes failed for logical model '{logical_model}'"]

        if not attempts:
            parts.append("No routes were available to try.")
        else:
            parts.append(f"Attempted {len(attempts)} route(s):")

            # Create error lookup by attempt number
            error_by_attempt = {e.get("attempt"): e for e in errors}

            for attempt in attempts:
                route = attempt.route
                error_info = error_by_attempt.get(attempt.attempt_number, {})
                error_msg = error_info.get("error", "Unknown error")
                error_type = error_info.get("error_type", "Unknown")

                fallback_marker = " [FALLBACK]" if attempt.is_fallback_route else ""
                status = (
                    f"FAILED ({error_type}: {error_msg[:80]}...)"
                    if len(error_msg) > 80
                    else f"FAILED ({error_type}: {error_msg})"
                )

                parts.append(
                    f"  {attempt.attempt_number}. {route.provider}/{route.model} "
                    f"({route.wire_protocol}){fallback_marker} - {status}"
                )

        return "\n".join(parts)


# Convenience functions for easy access


async def call_with_fallback(
    logical_model: str,
    request_data: Dict[str, Any],
    target_protocol: WireProtocol,
    stream: bool = False,
    max_key_cycles: Optional[int] = None,
) -> Any:
    """
    Convenience function to call with fallback routing.

    Creates a new FallbackRouter instance and executes the request.

    Args:
        logical_model: The logical model name
        request_data: Request payload in target_protocol format
        target_protocol: Expected response protocol ("openai" or "anthropic")
        stream: Whether to return a streaming response
        max_key_cycles: Maximum cycles through all keys per provider before
                       falling back to next provider. Defaults to MAX_KEY_RETRY_CYCLES.

    Returns:
        Response data or async generator for streaming
    """
    router = FallbackRouter()
    return await router.call_with_fallback(
        logical_model=logical_model,
        request_data=request_data,
        target_protocol=target_protocol,
        stream=stream,
        max_key_cycles=max_key_cycles,
    )


async def call_with_fallback_legacy(
    logical_model: str, exec_route_fn: Callable[[ResolvedRoute], Any]
) -> Any:
    """
    Legacy convenience function for backward compatibility.

    This maintains the old interface where a custom execution function is provided.
    Prefer using call_with_fallback() with the new interface.

    Args:
        logical_model: The logical model name to resolve
        exec_route_fn: Function to execute a resolved route, should return response or raise

    Returns:
        The successful response from the first working route

    Raises:
        RoutingError: If all routes fail
    """
    router = FallbackRouter()
    attempts = router.resolve_attempts(logical_model)

    errors = []
    for attempt in attempts:
        try:
            result = await exec_route_fn(attempt.route)
            return result
        except Exception as e:
            if router._is_fallback_worthy_error(e):
                errors.append(
                    {
                        "attempt": attempt.attempt_number,
                        "error": str(e),
                        "error_type": type(e).__name__,
                    }
                )
                continue
            else:
                raise

    raise RoutingError(
        logical_model=logical_model,
        attempted_routes=attempts,
        errors=errors,
        message=router._format_routing_error_message(logical_model, attempts, errors),
    )
